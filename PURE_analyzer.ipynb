{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_path = \"data\\\\Pure_Annotate_Dataset.csv\"\n",
    "pure_test_data_path = \"data\\\\testData.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqlist_path = os.path.join(\"data\", \"ReqList_ReqNet_ReqSim\", \"1.1 ReqLists\")\n",
    "req_list_files = os.listdir(reqlist_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#req_path = os.path.join(\"data\", \"req\")\n",
    "req_path = os.path.join(\"data\", \"ReqList_ReqNet_ReqSim\", \"0    Requirement Specification Documents\")\n",
    "req_doc_files = os.listdir(req_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de arquivos de requisitos em req_list_files: 86\n",
      "Número de documentos de requisitos em rec_doc_files: 82\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de arquivos de requisitos em req_list_files: {len(req_list_files)}\")\n",
    "print(f\"Número de documentos de requisitos em rec_doc_files: {len(req_doc_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2009 - gaia', '2022 - UAM IMS', 'light-control-system', '2004 - rlcs', '2007 - ertms', '1998 - themas', '2007 - eirene fun 7', '2001 - npac', '2007 - Wildlife', '2009 - peppol', '2002 - evla back', '2007 - get real 0', '2014 - Minutia', '2016 - FDP Clearinghouse', '2005 - znix', '2007 - nde', '2005 - triangle', '2005 - nenios', '1999 - multi-mahjong', '2000 - Barrel', '2003 - tachonet', '2005 - grid 3D', '2010 - blit draft', '0000 - cctns', '2003 - qheadache', '0000 - gamma j', '2002 - evla corr', '2004 - colorcast', '2001 - esa', '2011 - KMS', '1999 - tcs', 'automated-insulin-pump', '2000 - nasa x38', '2009 - library', '2010 - split merge', '2019 - MOSAR', 'RROWDyS', '2009 - video search', '2001 - ctc network', '2008 - virtual ed', '2010 - fishing', '2009 - warc III', '2001 - hats', '2007 - puget sound', '2004 - sprat', '2005 - phin', '2003 - agentmom', '1999 - Defense', '2006 - stewards', '2005 - clarus low', '2009 - library2', '2003 - pnnl', '2004 - grid bgc', '2021 - ReqView', '1997 - Modis', '2006 - eirene sys 15', '2017 - ePhyto', '2022 - MobileSurveillance', '2005 - clarus high', '2001 - beyond', '2008 - caiso', '2012 - NASAProcessReq', '2005 - pontis', '1995 - gemini', '1995 - Landsat7', '2010 - mashboot', '0000 - inventory', '2001 - elsfork', '2007 - e-store', '2007 - mdot', '2010 - home 1'}\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "req_list_files_names = set([x[:x.find(\".\")] for x in req_list_files])\n",
    "req_doc_files_names = set([x[:x.find(\".\")] for x in req_doc_files])\n",
    "    \n",
    "inter_list_doc = set(req_list_files_names).intersection(set(req_doc_files_names))\n",
    "print(inter_list_doc)\n",
    "print(len(inter_list_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos em req_list_files_names que não estão em req_doc_files_names: ['2004 - SGVTraffic', '2011 - CCHIT', '2012 - EMR CCHCS EA', '2012 - EMR CCHCS ISO', '2012 - EMR CCHIT LT', '2012 - EMR HHS', '2012 - EMR HL7 DC', '2012 - EMR HL7 IN', '2012 - EMR Pharmacy', '2013 - iTrust', '2017 - NISTMfgData', '2018 - DataWarehouse', '2021 - ConnectedVehiclePilotNYC', 'EHR System FuncReq - LA DHS', 'EHR System TechReq LA DHS']\n",
      "Arquivos em req_doc_files_names que não estão em req_list_files_names: ['2004 - SGVTF Integration', '2011 - CCHIT Ambulatory EHR 20110517', '2012 - EMR CCHCS EA ISO HL7 IN 20120420_Attach8', '2012 - EMR HL7 DC - CCHIT LT - Pharmacy - HHS_Attach9', '2013 - iTrust_requirements', '2017 - NISTMfgData NIST', '2018 - Data_Warehouse_V2_0', '2021 - Connected Vehicle Pilot NYC', 'EHR System FuncReq - LA DHS Appendix H-1', 'EHR System TechReq LA DHS Appendix I-1']\n"
     ]
    }
   ],
   "source": [
    "req_files_list_sub_doc = list(req_list_files_names - req_doc_files_names)\n",
    "req_files_doc_sub_list = list(req_doc_files_names - req_list_files_names)\n",
    "req_files_list_sub_doc.sort()\n",
    "req_files_doc_sub_list.sort() \n",
    "print(f\"Arquivos em req_list_files_names que não estão em req_doc_files_names: {req_files_list_sub_doc}\")\n",
    "print(f\"Arquivos em req_doc_files_names que não estão em req_list_files_names: {req_files_doc_sub_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "2004 - SGVTF Integration\n",
      "2011 - CCHIT Ambulatory EHR 20110517\n",
      "2012 - EMR CCHCS EA ISO HL7 IN 20120420_Attach8\n",
      "2012 - EMR HL7 DC - CCHIT LT - Pharmacy - HHS_Attach9\n",
      "2013 - iTrust_requirements\n",
      "2017 - NISTMfgData NIST\n",
      "2018 - Data_Warehouse_V2_0\n",
      "2021 - Connected Vehicle Pilot NYC\n",
      "EHR System FuncReq - LA DHS Appendix H-1\n",
      "EHR System TechReq LA DHS Appendix I-1\n"
     ]
    }
   ],
   "source": [
    "print(len(req_files_doc_sub_list))\n",
    "for file in req_files_doc_sub_list:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "2004 - SGVTraffic\n",
      "2011 - CCHIT\n",
      "2012 - EMR CCHCS EA\n",
      "2012 - EMR CCHCS ISO\n",
      "2012 - EMR CCHIT LT\n",
      "2012 - EMR HHS\n",
      "2012 - EMR HL7 DC\n",
      "2012 - EMR HL7 IN\n",
      "2012 - EMR Pharmacy\n",
      "2013 - iTrust\n",
      "2017 - NISTMfgData\n",
      "2018 - DataWarehouse\n",
      "2021 - ConnectedVehiclePilotNYC\n",
      "EHR System FuncReq - LA DHS\n",
      "EHR System TechReq LA DHS\n"
     ]
    }
   ],
   "source": [
    "print(len(req_files_list_sub_doc))\n",
    "for file in req_files_list_sub_doc:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_para = {\"2004 - SGVTF Integration\" : \"2004 - SGVTraffic\",           \n",
    "        \"2011 - CCHIT Ambulatory EHR 20110517\" : \"2011 - CCHIT\",\n",
    "        \"2012 - EMR CCHCS EA ISO HL7 IN 20120420_Attach8\" : [\"2012 - EMR CCHCS EA\", \"2012 - EMR CCHCS ISO\", \"2012 - EMR HL7 IN\"],\n",
    "        \"2012 - EMR HL7 DC - CCHIT LT - Pharmacy - HHS_Attach9\" : [\"2012 - EMR CCHIT LT\", \"2012 - EMR HHS\", \"2012 - EMR HL7 DC\", \"2012 - EMR Pharmacy\"],\n",
    "        \"2013 - iTrust_requirements\" : \"2013 - iTrust\",\n",
    "        \"2017 - NISTMfgData NIST\" : \"2017 - NISTMfgData\",\n",
    "        \"2018 - Data_Warehouse_V2_0\" : \"2018 - DataWarehouse\",\n",
    "        \"2021 - Connected Vehicle Pilot NYC\" : \"2021 - ConnectedVehiclePilotNYC\",\n",
    "        \"EHR System FuncReq - LA DHS Appendix H-1\" : \"EHR System FuncReq - LA DHS\",\n",
    "        \"EHR System TechReq LA DHS Appendix I-1\": \"EHR System TechReq LA DHS\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF gerado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "caminho_pdf_original = \"Software_Requirements_3rd_Edition.pdf\"\n",
    "caminho_pdf_saida = \"Understanding_user_requirements.pdf\"\n",
    "\n",
    "\n",
    "reader = PdfReader(caminho_pdf_original)\n",
    "writer = PdfWriter()\n",
    "\n",
    "\n",
    "for i in range(175, 199):  \n",
    "    writer.add_page(reader.pages[i])\n",
    "\n",
    "with open(caminho_pdf_saida, \"wb\") as f:\n",
    "    writer.write(f)\n",
    "\n",
    "print(\"PDF gerado com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list_req_path = \"data/ReqList_ReqNet_ReqSim/1.2 all_reqlists.csv\"\n",
    "df_all_list_req = pd.read_csv(all_list_req_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segundos: 38103\n",
      "Minutos: 1143.09\n",
      "Horas: 19.0515\n",
      "Dias: 0.7938125\n"
     ]
    }
   ],
   "source": [
    "print(f\"Segundos: {df_all_list_req.size}\")\n",
    "print(f\"Minutos: {(df_all_list_req.size*3)/100}\")\n",
    "print(f\"Horas: {(df_all_list_req.size*3)/60/100}\")\n",
    "print(f\"Dias: {(df_all_list_req.size*3)/60/100/24}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = df_all_list_req[\"requirement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = df_all_list_req[\"keys\"]\n",
    "\n",
    "keys_0 = []\n",
    "keys_1 = []\n",
    "keys_2 = []\n",
    "keys_3 = []\n",
    "keys_4 = []\n",
    "keys_5 = []\n",
    "keys_6 = []\n",
    "keys_7 = []\n",
    "keys_8 = [] \n",
    "keys_9 = []\n",
    "keys_10 = []\n",
    "keys_11 = []\n",
    "keys_list = [keys_1, keys_2, keys_3, keys_4, keys_5, keys_6, keys_7, keys_8, keys_9, keys_10, keys_11]\n",
    "\n",
    "for key in keys:\n",
    "    key_0 = \".\".join(key.split(\".\")[:2])\n",
    "    key_rest = key.split(\".\")[2:]\n",
    "    for i in range(11):\n",
    "        try:\n",
    "            keys_list[i].append(key_rest[i])\n",
    "        except:\n",
    "            keys_list[i].append(\"\")\n",
    "    keys_0.append(key_0)\n",
    "df_all_list_req[\"keys_0\"] = keys_0\n",
    "df_all_list_req[\"keys_1\"] = keys_1\n",
    "df_all_list_req[\"keys_2\"] = keys_2\n",
    "df_all_list_req[\"keys_3\"] = keys_3\n",
    "df_all_list_req[\"keys_4\"] = keys_4\n",
    "df_all_list_req[\"keys_5\"] = keys_5\n",
    "df_all_list_req[\"keys_6\"] = keys_6\n",
    "df_all_list_req[\"keys_7\"] = keys_7\n",
    "df_all_list_req[\"keys_8\"] = keys_8\n",
    "df_all_list_req[\"keys_9\"] = keys_9\n",
    "df_all_list_req[\"keys_10\"] = keys_10\n",
    "df_all_list_req[\"keys_11\"] = keys_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in keys_9:\n",
    "    if i != \"\":\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "reqs_parsed = []\n",
    "\n",
    "for req in requirements:\n",
    "    req_parsed = nlp(req)\n",
    "    reqs_parsed.append(req_parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_list_req[\"parsed\"] = reqs_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb = [] # Verbo (should provide, enable, etc.)\n",
    "aux = [] # Auxiliar (should, must, can, is, are, etc.)\n",
    "noun = [] # Substantivo (system, user, report, etc.)\n",
    "propn = [] # Substantivo próprio (Nome da empresa, nome de funcionário, etc.)\n",
    "pron = [] # Pronome (user, it, they, he, she, etc.)\n",
    "adj = [] # Adjetivo (detailed, new, old, etc.)\n",
    "adv = [] # Advérbio (automatically, only, quickly, slowly, etc.)\n",
    "sconj = [] # Conjunção subordinada (so, that, when, if, because, etc.)\n",
    "part = [] # Particípio (to provide, to allow, should be, should have, etc.)\n",
    "org = [] # Organização (Nome da empresa, nome de funcionário, etc.)\n",
    "\n",
    "for req in reqs_parsed:\n",
    "    verb.append([token for token in req if token.pos_ == \"VERB\"])\n",
    "    aux.append([token for token in req if token.pos_ == \"AUX\"])\n",
    "    noun.append([token for token in req if token.pos_ == \"NOUN\"])\n",
    "    propn.append([token for token in req if token.pos_ == \"PROPN\"])\n",
    "    pron.append([token for token in req if token.pos_ == \"PRON\"])\n",
    "    adj.append([token for token in req if token.pos_ == \"ADJ\"])\n",
    "    adv.append([token for token in req if token.pos_ == \"ADV\"])\n",
    "    sconj.append([token for token in req if token.pos_ == \"SCONJ\"])\n",
    "    part.append([token for token in req if token.pos_ == \"PART\"])\n",
    "    org.append([token for token in req if token.pos_ == \"ORG\"])\n",
    "        \n",
    "df_all_list_req[\"verb\"] = verb\n",
    "df_all_list_req[\"aux\"] = aux\n",
    "df_all_list_req[\"noun\"] = noun\n",
    "df_all_list_req[\"propn\"] = propn\n",
    "df_all_list_req[\"pron\"] = pron\n",
    "df_all_list_req[\"adj\"] = adj\n",
    "df_all_list_req[\"adv\"] = adv\n",
    "df_all_list_req[\"sconj\"] = sconj\n",
    "df_all_list_req[\"part\"] = part\n",
    "df_all_list_req[\"org\"] = org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '1', '2', '0', '3', '4', '5', '6', '7', '8', '9', '10', '11',\n",
       "       '12', '13', '14', '15', '16', '1 ', '17', '18', '19', '20', '21',\n",
       "       '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
       "       '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43',\n",
       "       '44', '45', '46', '47', '48', '49', '50', '51', '218', '52', '53',\n",
       "       '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64',\n",
       "       '65', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77',\n",
       "       '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n",
       "       '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99',\n",
       "       '100', '101', '102', '103', '104', '105', '106', '107', '108',\n",
       "       '109', '110', '111', '112', '228', '113', '114', '115', '116',\n",
       "       '117', '118', '119', '120', '121', '122', '123', '124', '125',\n",
       "       '126', '127', '128', '129', '130', '131', '132', '133', '134',\n",
       "       '135', '136', '137', '138', '139', '140', '141', '142', '143',\n",
       "       '144', '145', '146', '147', '148', '149', '150', '151', '152',\n",
       "       '153', '154', '155', '156', '157', '158', '159', '160', '161',\n",
       "       '162', '163', '164', '165', '166', '167', '168', '169', '170',\n",
       "       '171', '172', '173', '174', '175', '176', '177', '178', '179',\n",
       "       '180', '181', '182', '183', '184', '185', '186', '187', '188',\n",
       "       '189', '190', '191', '192', '193', '194', '195', '196', '197',\n",
       "       '198', '199', '200', '201', '202', '204', '205', '206', '207',\n",
       "       '211', '213', '214', '216', '217', '215', '221', '222', '223',\n",
       "       '231', '241', '245', '251', '255', '261', '271', '275', '281',\n",
       "       '401', '501', '505', '521', '801', '805', '806', '811', '901',\n",
       "       '905', '301', '219', '209', '225', '232', '233', '235', '237',\n",
       "       '239', '305', '311', '315', '601', '605', '611', '615', '815',\n",
       "       '823', '825', '827', '910', '915', '921', '925', '931', '935',\n",
       "       '66', '67', '02', '03', '203', '302', '303', '304', '402', '403',\n",
       "       '404', '405', '502', '602', '603', '604', '606', '607', '701',\n",
       "       '702', '703', '704', '705', '706', '707', '708', '709', '710',\n",
       "       '711', '712', '713', '714', '715', '802', '306', '406', '407',\n",
       "       '408', '800', '803', '307', '409', '410', '411'], dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_list_req[\"keys_4\"].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
